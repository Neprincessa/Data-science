---
title: "lab6"
author: "Anastasia Khudoyarova"
date: "11/30/2020"
output:
  rmarkdown::github_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Load

```{r}
data_f <- as.data.frame(read.table('zeta.csv', header = TRUE, sep=','))
data_f <- data_f[data_f$sex == "F",]
data_f <- subset.data.frame(data_f, select = c("meanage","meaneducation","meanemployment","meanhouseholdincome"))
head(data_f)
```


### Filter as:
`8 < meaneducation < 18`
`10,000 < meanhouseholdincome < 200,000`
`0 < meanemployment < 3`
`20 < meanage < 60`


```{r}
c_df <-subset(data_f, 
           8 < data_f$meaneducation & 
           data_f$meaneducation < 18 & 
           10000 < data_f$meanhouseholdincome & 
           data_f$meanhouseholdincome < 200000 & 
           0 < data_f$meanemployment & 
           data_f$meanemployment < 3 & 
           20 < data_f$meanage & 
           data_f$meanage < 60)
head(c_df)
```


Create a variable called log_income = log10(meanhouseholdincome)

```{r}
c_df <- cbind(c_df, log10(c_df$meanhouseholdincome))
head(c_df, n=10)
```

Rename the columns meanage, meaneducation, and meanemployment as age, education, and employment, respectively

```{r}
colnames(c_df) <- c("age", "education", "employment", "income", "log_income")
head(c_df)
```


# Linear Regression Analysis

### a. Scatter plot that shows the effect age on log_income. There is almost no relationship.

```{r}
library(hexbin)
hexbinplot(c_df$log_income ~ c_df$age, trans = sqrt, inv = function(x) x^2, xlab="Age", ylab="Log_Income", colramp=rainbow)
```

### b. Linear regression model between log_income and age.  

t-value показывает насколько данные удобны для создания выборки, то есть если значение мало, то данные распределены компактно и мы можем как - то их анализировать; если значение велико, то значит, данные слишком разрозненные.
Связь t value и p value: Чем выше значение t value и ниже p, тем более вероятно, что нет null hypothesis, потому что низкая вероятность ошибки при нашей гипотезе.
А данном случае проверяя статистическую значимость, понимаем, что знаение p выше порогового значения t, что говорит о том, что мы не можем отколонить нулевую гипотезу.Следовательно, данная модель не описывает достаточно точно систему.

```{r}
lin_reg1 <- lm(formula=log_income ~ age, data=c_df)
summary(lin_reg1)
```


### c. R-squared измеряет силу связи между моделью и зависимым значением по шкале от 0 до 100%
Чем больше это значение, тем лучше модель подходит для исследования. Потому что R2= 1-(дисперсия ошибок модели) -> чем ближе к 1 значение, тем меньше ошибок в регрессионной модели.Если R2 < 0.5, то данная модель не описывает нормально нашу систему.

### d. F-statistic показывает можем ли мы отвергнуть null hypothesis. То есть случай когда все параметры нулевые
Чтобы получить хорошую модель F-statistic должна быть больше 1, а p-value очень маленьким.

### e. Multiple R-squared:  0.01184,	Adjusted R-squared:  0.01181
R-value около 0. Модель не очень хорошая.

### f. Scatter plot that shows the effect education has on log_income. 

```{r}
hexbinplot(c_df$education ~ c_df$log_income, xlab="Education", ylab="Log_Income", trans = sqrt, inv = function(x) x^2, type = c("g","r"), colramp=rainbow)
```

### g. Summary of a linear regression model between log_income and education.  

Multiple R-squared:  0.5354,	Adjusted R-squared:  0.5354 

В этом случае у нас R-squared ближе к 1 => лучше, чем прошлая модель 

```{r}
lin_reg2 <- lm(formula=log_income ~ education, data=c_df)
summary(lin_reg2)
```

### h. Summary of a linear regression model between the dependent variable log_income, and the independent variables age, education, and employment

```{r}
lin_reg3 <- lm(formula=log_income ~ age + education + employment, data=c_df)
summary(lin_reg3)
```

### i. С каждым шагом образования доход растет на 9 %

### j. Graph that contains a y = x line and uses the multiple regression model to plot the predicted data points against the actual data points of the training set. 

```{r}
c_df["predict"] <- predict(lin_reg3, c_df)
hexbinplot(c_df$log_income ~ c_df$predict, ylab="Log_income", xlab="Predicted data", trans = sqrt, inv = function(x) x^2, type = c("g","r"), colramp=rainbow)
```

### k. Как видно на графике, линия y=x почти повторяет разброс значений. Так же ярко видна область, где точки как бы облепливают прямую y=x
Это означает, что модель действительно хорошая и предсказание совпадает с реальностью
